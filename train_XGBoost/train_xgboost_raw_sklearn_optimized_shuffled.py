
import numpy as np
from xgboost import XGBClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import f1_score
from sklearn.utils import shuffle
import joblib

# ==== è¶…åƒæ•¸è¨­å®šï¼ˆæœ€ä½³å¯¦å‹™å»ºè­°ï¼‰====
LEARNING_RATE = 0.01
MAX_DEPTH = 8
MIN_CHILD_WEIGHT = 1
GAMMA = 0.01
SUBSAMPLE = 0.8
COLSAMPLE_BYTREE = 0.8
N_ESTIMATORS = 400
EARLY_STOPPING_ROUNDS = 20
TREE_METHOD = "hist"
GROW_POLICY = "lossguide"
SCALE_POS_WEIGHT = 9.5  
N_JOBS = 8
RANDOM_SEED = 42
TEST_SIZE = 0.1

def train_xgboost(X_path="training_dataset_procced/X_full.npy", y_path="training_dataset_procced/y.npy"):
    print("ðŸ“¦ è¼‰å…¥è³‡æ–™...")
    X = np.load(X_path)
    y = np.load(y_path)

    print("ðŸ”€ æ‰“æ•£è³‡æ–™ä¸­ï¼ˆshuffleï¼‰...")
    X, y = shuffle(X, y, random_state=RANDOM_SEED)

    print("ðŸ“Š åˆ†å‰²è¨“ç·´ / é©—è­‰é›†...")
    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=TEST_SIZE, stratify=y, random_state=RANDOM_SEED)

    print("ðŸ§  é–‹å§‹è¨“ç·´ XGBClassifier...")
    model = XGBClassifier(
        objective="binary:logistic",
        eval_metric="logloss",
        learning_rate=LEARNING_RATE,
        max_depth=MAX_DEPTH,
        min_child_weight=MIN_CHILD_WEIGHT,
        gamma=GAMMA,
        subsample=SUBSAMPLE,
        colsample_bytree=COLSAMPLE_BYTREE,
        n_estimators=N_ESTIMATORS,
        early_stopping_rounds=EARLY_STOPPING_ROUNDS,
        scale_pos_weight= sum(y == 0) / sum(y == 1),
        tree_method=TREE_METHOD,
        grow_policy=GROW_POLICY,
        n_jobs=N_JOBS,
        verbosity=1,
        random_state=RANDOM_SEED,
        use_label_encoder=False
    )

    model.fit(
        X_train, y_train,
        eval_set=[(X_val, y_val)],
        verbose=True
    )

    joblib.dump(model, "xgb_raw_model.pkl")
    print("å·²å„²å­˜ç‚ºï¼šxgb_raw_model.pkl")

    print("é æ¸¬è¨“ç·´è³‡æ–™...")
    probs = model.predict_proba(X)[:, 1]
    np.save("p_xgb.npy", probs)
    print(f"å·²å„²å­˜ï¼šp_xgb.npy, shape = {probs.shape}")

    # è¨ˆç®—æœ€ä½³ F1-score
    best_f1, best_t = 0, 0
    for t in np.arange(0.1, 0.9, 0.01):
        preds = (probs > t).astype(int)
        f1 = f1_score(y, preds)
        if f1 > best_f1:
            best_f1, best_t = f1, t
    print(f"æœ€ä½³ threshold = {best_t:.2f}, F1-score = {best_f1:.4f}")

if __name__ == "__main__":
    train_xgboost()
